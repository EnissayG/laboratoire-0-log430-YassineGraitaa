## ğŸ“¡ 1.2 ObservabilitÃ© de base

Dans cette section, nous avons instrumentÃ© lâ€™API FastAPI afin de collecter et visualiser des mÃ©triques via Prometheus et Grafana.  
Lâ€™objectif est dâ€™observer les **4 Golden Signals** dÃ©crits par les pratiques SRE de Google : **latence, trafic, erreurs et saturation**.

Un endpoint `/metrics` a Ã©tÃ© exposÃ© grÃ¢ce Ã  `prometheus_fastapi_instrumentator`, et les donnÃ©es sont collectÃ©es toutes les 5 secondes par Prometheus. Grafana est utilisÃ© pour lâ€™exploration et lâ€™affichage graphique.

---

### ğŸ”§ Mise en place technique

- ğŸ“¦ Exportation des mÃ©triques via `prometheus_fastapi_instrumentator`
- ğŸ³ DÃ©ploiement de Prometheus et Grafana avec `docker-compose`
- ğŸ” Test de charge gÃ©nÃ©rÃ© avec `k6` pour simuler du trafic
- ğŸ“Š Dashboard Grafana personnalisÃ©, incluant les 4 signaux

---

### ğŸ“ˆ Golden Signals observÃ©s

#### 1. **Latence moyenne**

La latence est mesurÃ©e par la division de `rate(http_request_duration_seconds_sum)` sur `rate(http_request_duration_seconds_count)`, agrÃ©gÃ©e par `handler`.  
Cela nous donne un aperÃ§u prÃ©cis du temps moyen de traitement par endpoint.

![Latence moyenne](images/latence_moyenne.png)

> ğŸ’¬ On observe une latence moyenne relativement stable (~120â€“350â€¯ms), avec une hausse lors des tests de stress.

---

#### 2. **Trafic HTTP**

Le trafic est mesurÃ© en **requÃªtes par seconde (RPS)** Ã  lâ€™aide de :
```promql
sum by (handler) (rate(http_requests_total[1m]))
```

![Trafic HTTP](images/trafic_http.png)

> ğŸ’¬ Le volume de requÃªtes augmente fortement durant les phases de test (jusqu'Ã  ~120 RPS).

---

#### 3. **Erreurs HTTP**

Les erreurs 4xx (utilisateur) et 5xx (serveur) sont agrÃ©gÃ©es sÃ©parÃ©ment.

- 4xx :
```promql
sum by (handler) (rate(http_requests_total{status=~"4.."}[1m]))
```
- 5xx :
```promql
rate(http_requests_total{status=~"5.."}[1m])
```

![Erreurs 4xx](images/erreurs_4xx.png)  
![Erreurs 5xx](images/erreurs_5xx.png)

> ğŸ’¬ Les erreurs 422 et 404 ont Ã©tÃ© gÃ©nÃ©rÃ©es volontairement lors des tests pour valider la robustesse des routes.

---

#### 4. **Saturation (CPU)**

Nous avons utilisÃ© :
```promql
rate(process_cpu_seconds_total[1m])
```
pour estimer la charge CPU du processus FastAPI.

![Saturation CPU](images/saturation_cpu.png)

> ğŸ’¬ Une montÃ©e progressive de l'utilisation CPU est visible lors des tests de stress (jusqu'Ã  0.8s/s â‰ˆ 80% dâ€™un cÅ“ur CPU).

---

### âœ… RÃ©sultat

Lâ€™observabilitÃ© est en place et a permis de :
- Diagnostiquer les limites de performance avant optimisations
- Quantifier les effets des tests de charge
- PrÃ©parer les bases pour lâ€™Ã©tape suivante : **mise en place dâ€™un Load Balancer**



## ğŸ” Analyse des points faibles de lâ€™architecture

Lâ€™analyse des mÃ©triques collectÃ©es via Prometheus et visualisÃ©es dans Grafana a permis dâ€™identifier plusieurs points sensibles au sein de notre systÃ¨me actuel :

---

### 1. Goulets dâ€™Ã©tranglement observÃ©s

#### ğŸ”¸ Endpoint lent : `/api/ventes/rapport`

La latence moyenne mesurÃ©e sur ce point dâ€™accÃ¨s sâ€™Ã©levait Ã  plus de **350 ms** en p95 durant les tests de stress. Ce comportement indique une requÃªte complexe, probablement causÃ©e par :
- des **agrÃ©gations SQL** coÃ»teuses (`GROUP BY`, `JOIN`)
- un **volume de donnÃ©es croissant** dans les tables `ventes`, `produits` ou `magasins`
- lâ€™absence dâ€™un **mÃ©canisme de cache**

#### ğŸ”¸ Endpoint `/api/magasins/{id}/stock`

Bien que performant en moyenne, cet endpoint a montrÃ© une **hausse de la latence** Ã  fort trafic. Cela peut indiquer :
- des **requÃªtes SQL non indexÃ©es** sur les colonnes `magasin_id` ou `produit_id`
- un **pool de connexions limitÃ©** cÃ´tÃ© backend ou base de donnÃ©es

---

### 2. Taux dâ€™erreurs observÃ©

GrÃ¢ce aux requÃªtes filtrÃ©es sur les `status=~"4.."` et `status=~"5.."`, on a identifiÃ© :
- des erreurs 422 volontaires gÃ©nÃ©rÃ©es via des requÃªtes invalides (expected âœ…)
- aucun 5xx en temps normal, ce qui indique **une bonne stabilitÃ© serveur**

Cependant, il conviendrait d'**anticiper les cas de charge extrÃªme** oÃ¹ les ressources serveur ou la base pourraient devenir indisponibles.

---

### 3. Saturation progressive

Le panel de saturation basÃ© sur :
```promql
rate(process_cpu_seconds_total[1m])
```
montre une montÃ©e significative du CPU vers **0.7s/s** (soit ~70 % d'un cÅ“ur) lors des tests Ã  100 utilisateurs virtuels (VU). Cela indique une **proximitÃ© du point de rupture**.

---

### âœ… Recommandations (sans augmenter les ressources)

| AmÃ©lioration | DÃ©tail |
|--------------|--------|
| âœ… **Indexation** | Ajouter des index sur `magasin_id`, `produit_id`, `date` dans les tables `ventes` et `stocks` |
| âœ… **RequÃªtes SQL** | Optimiser les requÃªtes de `/rapports` pour limiter les agrÃ©gations inutiles |
| âœ… **Cache applicatif** | Mettre en cache les rÃ©sultats de `/rapports` ou `/stock` via Redis ou cache mÃ©moire |
| âœ… **Taille du pool SQLAlchemy** | Augmenter la taille du pool de connexions si tu observes des blocages frÃ©quents |
| âœ… **PrÃ©voir le load balancing** | Ã‰tape suivante du labo : NGINX + plusieurs instances FastAPI |

---

L'ensemble de ces actions permettra de **mieux prÃ©parer l'application Ã  la montÃ©e en charge**, tout en conservant la mÃªme infrastructure de base.

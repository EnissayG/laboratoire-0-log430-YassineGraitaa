## ğŸ“¡ 1.2 ObservabilitÃ© de base

Dans cette section, nous avons instrumentÃ© lâ€™API FastAPI afin de collecter et visualiser des mÃ©triques via Prometheus et Grafana.  
Lâ€™objectif est dâ€™observer les **4 Golden Signals** dÃ©crits par les pratiques SRE de Google : **latence, trafic, erreurs et saturation**.

Un endpoint `/metrics` a Ã©tÃ© exposÃ© grÃ¢ce Ã  `prometheus_fastapi_instrumentator`, et les donnÃ©es sont collectÃ©es toutes les 5 secondes par Prometheus. Grafana est utilisÃ© pour lâ€™exploration et lâ€™affichage graphique.

---

### ğŸ”§ Mise en place technique

- ğŸ“¦ Exportation des mÃ©triques via `prometheus_fastapi_instrumentator`
- ğŸ³ DÃ©ploiement de Prometheus et Grafana avec `docker-compose`
- ğŸ” Test de charge gÃ©nÃ©rÃ© avec `k6` pour simuler du trafic
- ğŸ“Š Dashboard Grafana personnalisÃ©, incluant les 4 signaux

---

### ğŸ“ˆ Golden Signals observÃ©s

#### 1. **Latence moyenne**

La latence est mesurÃ©e par la division de `rate(http_request_duration_seconds_sum)` sur `rate(http_request_duration_seconds_count)`, agrÃ©gÃ©e par `handler`.  
Cela nous donne un aperÃ§u prÃ©cis du temps moyen de traitement par endpoint.

![Latence moyenne](images/latence_moyenne.png)

> ğŸ’¬ On observe une latence moyenne relativement stable (~120â€“350â€¯ms), avec une hausse lors des tests de stress.

---

#### 2. **Trafic HTTP**

Le trafic est mesurÃ© en **requÃªtes par seconde (RPS)** Ã  lâ€™aide de :
```promql
sum by (handler) (rate(http_requests_total[1m]))
```

![Trafic HTTP](images/trafic_http.png)

> ğŸ’¬ Le volume de requÃªtes augmente fortement durant les phases de test (jusqu'Ã  ~120 RPS).

---

#### 3. **Erreurs HTTP**

Les erreurs 4xx (utilisateur) et 5xx (serveur) sont agrÃ©gÃ©es sÃ©parÃ©ment.

- 4xx :
```promql
sum by (handler) (rate(http_requests_total{status=~"4.."}[1m]))
```
- 5xx :
```promql
rate(http_requests_total{status=~"5.."}[1m])
```

![Erreurs 4xx](images/erreurs_4xx.png)  
![Erreurs 5xx](images/erreurs_5xx.png)

> ğŸ’¬ Les erreurs 422 et 404 ont Ã©tÃ© gÃ©nÃ©rÃ©es volontairement lors des tests pour valider la robustesse des routes.

---

#### 4. **Saturation (CPU)**

Nous avons utilisÃ© :
```promql
rate(process_cpu_seconds_total[1m])
```
pour estimer la charge CPU du processus FastAPI.

![Saturation CPU](images/saturation_cpu.png)

> ğŸ’¬ Une montÃ©e progressive de l'utilisation CPU est visible lors des tests de stress (jusqu'Ã  0.8s/s â‰ˆ 80% dâ€™un cÅ“ur CPU).

---

### âœ… RÃ©sultat

Lâ€™observabilitÃ© est en place et a permis de :
- Diagnostiquer les limites de performance avant optimisations
- Quantifier les effets des tests de charge
- PrÃ©parer les bases pour lâ€™Ã©tape suivante : **mise en place dâ€™un Load Balancer**



## ğŸ” Analyse des points faibles de lâ€™architecture

Lâ€™analyse des mÃ©triques collectÃ©es via Prometheus et visualisÃ©es dans Grafana a permis dâ€™identifier plusieurs points sensibles au sein de notre systÃ¨me actuel :

---

### 1. Goulets dâ€™Ã©tranglement observÃ©s

#### ğŸ”¸ Endpoint lent : `/api/ventes/rapport`

La latence moyenne mesurÃ©e sur ce point dâ€™accÃ¨s sâ€™Ã©levait Ã  plus de **350 ms** en p95 durant les tests de stress. Ce comportement indique une requÃªte complexe, probablement causÃ©e par :
- des **agrÃ©gations SQL** coÃ»teuses (`GROUP BY`, `JOIN`)
- un **volume de donnÃ©es croissant** dans les tables `ventes`, `produits` ou `magasins`
- lâ€™absence dâ€™un **mÃ©canisme de cache**

#### ğŸ”¸ Endpoint `/api/magasins/{id}/stock`

Bien que performant en moyenne, cet endpoint a montrÃ© une **hausse de la latence** Ã  fort trafic. Cela peut indiquer :
- des **requÃªtes SQL non indexÃ©es** sur les colonnes `magasin_id` ou `produit_id`
- un **pool de connexions limitÃ©** cÃ´tÃ© backend ou base de donnÃ©es

---

### 2. Taux dâ€™erreurs observÃ©

GrÃ¢ce aux requÃªtes filtrÃ©es sur les `status=~"4.."` et `status=~"5.."`, on a identifiÃ© :
- des erreurs 422 volontaires gÃ©nÃ©rÃ©es via des requÃªtes invalides (expected âœ…)
- aucun 5xx en temps normal, ce qui indique **une bonne stabilitÃ© serveur**

Cependant, il conviendrait d'**anticiper les cas de charge extrÃªme** oÃ¹ les ressources serveur ou la base pourraient devenir indisponibles.

---

### 3. Saturation progressive

Le panel de saturation basÃ© sur :
```promql
rate(process_cpu_seconds_total[1m])
```
montre une montÃ©e significative du CPU vers **0.7s/s** (soit ~70 % d'un cÅ“ur) lors des tests Ã  100 utilisateurs virtuels (VU). Cela indique une **proximitÃ© du point de rupture**.

---

### âœ… Recommandations (sans augmenter les ressources)

| AmÃ©lioration | DÃ©tail |
|--------------|--------|
| âœ… **Indexation** | Ajouter des index sur `magasin_id`, `produit_id`, `date` dans les tables `ventes` et `stocks` |
| âœ… **RequÃªtes SQL** | Optimiser les requÃªtes de `/rapports` pour limiter les agrÃ©gations inutiles |
| âœ… **Cache applicatif** | Mettre en cache les rÃ©sultats de `/rapports` ou `/stock` via Redis ou cache mÃ©moire |
| âœ… **Taille du pool SQLAlchemy** | Augmenter la taille du pool de connexions si tu observes des blocages frÃ©quents |
| âœ… **PrÃ©voir le load balancing** | Ã‰tape suivante du labo : NGINX + plusieurs instances FastAPI |

---

L'ensemble de ces actions permettra de **mieux prÃ©parer l'application Ã  la montÃ©e en charge**, tout en conservant la mÃªme infrastructure de base.


## ğŸ” 2.1 Tests de charge via le Load Balancer

Dans cette Ã©tape, nous avons dÃ©ployÃ© un rÃ©partiteur de charge (NGINX) devant plusieurs instances de notre API FastAPI.  
Lâ€™objectif est de mesurer l'impact de la scalabilitÃ© horizontale sur les performances globales du systÃ¨me, et de valider la rÃ©silience en cas de panne dâ€™instance.

Les tests de charge sont identiques Ã  ceux effectuÃ©s dans lâ€™Ã©tape 1, mais dirigÃ©s cette fois-ci vers le load balancer via lâ€™URL :

```
http://localhost:8081
```

---

### âš™ï¸ ScÃ©narios testÃ©s

Nous avons simulÃ© 3 configurations diffÃ©rentes :

| Configuration | DÃ©tail |
|---------------|--------|
| N = 1 instance | `fastapi1` uniquement |
| N = 2 instances | `fastapi1` + `fastapi2` |
| N = 3 instances *(optionnel)* | `fastapi1` + `fastapi2` + `fastapi3` |

Pour chaque configuration, nous avons rÃ©pÃ©tÃ© les tests de charge (via K6) sur les endpoints suivants :
- `/api/magasins/1/stock`
- `/api/ventes/rapport`
- `/api/produits/1`

---

## ğŸ“Š Tests de charge avec Load Balancer

Nous avons simulÃ© une montÃ©e en charge sur lâ€™endpoint critique `/api/magasins/1/stock` via le port du Load Balancer (`http://host.docker.internal:8081`). Lâ€™objectif est de comparer les performances en faisant varier le nombre dâ€™instances FastAPI actives (N = 3, 2, 1).

### âš™ï¸ Configuration du test

```js
// test.js - Script K6 utilisÃ© pour la simulation
import http from 'k6/http';
import { check } from 'k6';

export let options = {
  stages: [
    { duration: '15s', target: 10 },
    { duration: '15s', target: 30 },
    { duration: '15s', target: 60 },
    { duration: '15s', target: 100 },
    { duration: '15s', target: 0 },
  ],
};

export default function () {
  let res = http.get('http://host.docker.internal:8081/api/magasins/1/stock', {
    headers: { 'x-token': 'mon-token-secret' },
  });
  check(res, { 'status is 200': (r) => r.status === 200 });
}
```

### ğŸ§ª Tests effectuÃ©s

| Test  | Instances actives         | Commande exÃ©cutÃ©e                           |
|-------|---------------------------|---------------------------------------------|
| N = 3 | `fastapi1`, `fastapi2`, `fastapi3` | `k6 run test.js` |
| N = 2 | `fastapi1`, `fastapi2`            | `docker stop fastapi3` puis `k6 run test.js` |
| N = 1 | `fastapi1`                        | `docker stop fastapi2` puis `k6 run test.js` |

### ğŸ“ˆ RÃ©sultats (via Grafana)

![alt text](./images/load_balance_test_3N.png)

- **Premier pic (~15:07)** : N = 3
- **DeuxiÃ¨me pic (~15:19)** : N = 2
- **TroisiÃ¨me pic (~15:22)** : N = 1

On observe une **augmentation de la latence moyenne** et une **baisse du throughput** Ã  mesure que lâ€™on rÃ©duit le nombre dâ€™instances.

### ğŸ§  InterprÃ©tation

- Le load balancer NGINX distribue correctement les requÃªtes sur les instances disponibles.
- Lorsque le nombre dâ€™instances diminue, la **charge par instance augmente**, ce qui se traduit par :
  - Une **latence plus Ã©levÃ©e**
  - Un **taux d'erreurs potentiellement accru** (Ã  vÃ©rifier)
  - Une **saturation CPU plus marquÃ©e** si observÃ©e
- Cela montre lâ€™importance de la **scalabilitÃ© horizontale** dans une architecture distribuÃ©e.


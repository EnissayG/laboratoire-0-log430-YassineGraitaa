# 1. Introduction et motivation

Ce document prÃ©sente lâ€™architecture du **microservice `orchestrateur-service`**, dÃ©veloppÃ© dans le cadre du **Laboratoire 6** du cours **LOG430 â€“ Architecture logicielle** (Ã‰TS, Ã©tÃ© 2025). Ce laboratoire sâ€™inscrit dans la continuitÃ© des prÃ©cÃ©dents (Labos 3, 4 et 5) oÃ¹ une application de gestion multi-magasins a Ã©voluÃ© dâ€™un monolithe vers une architecture distribuÃ©e Ã  base de microservices, orchestrÃ©e par une API Gateway.

Le `orchestrateur-service` a pour rÃ´le principal de **coordonner lâ€™exÃ©cution de commandes complexes** impliquant plusieurs services (panier, stock, client, ventes). Il implÃ©mente une **saga orchestrÃ©e synchrone**, permettant de garantir la cohÃ©rence globale mÃªme en cas dâ€™Ã©chec partiel. Il encapsule :

- La **gestion explicite de lâ€™Ã©tat dâ€™une commande**, sous forme de machine dâ€™Ã©tat (Enum, persistance SQL).
- La **compensation automatique** en cas dâ€™Ã©chec dâ€™une Ã©tape (ex. libÃ©ration du stock, remboursement).
- Lâ€™enregistrement **historique des transitions** pour la traÃ§abilitÃ© (`EtatSaga`).
- Lâ€™**exposition de mÃ©triques Prometheus** pour lâ€™observabilitÃ© (par Ã©tat de commande).
- Une interface API REST simple (checkout, consultation).

Lâ€™introduction de cette orchestration est cruciale dans un systÃ¨me distribuÃ©, oÃ¹ chaque microservice est indÃ©pendant et peut Ã©chouer : elle permet de centraliser la logique de coordination tout en respectant le principe de dÃ©couplage.

Ce rapport documente donc le design, les responsabilitÃ©s, les dÃ©cisions techniques et les interactions de ce microservice clÃ© dans lâ€™Ã©cosystÃ¨me du systÃ¨me multi-magasins.

# 2. Contraintes

Le `orchestrateur-service` a Ã©tÃ© dÃ©veloppÃ© sous plusieurs contraintes fonctionnelles, techniques et dâ€™infrastructure, dÃ©finies par le contexte du projet LOG430 â€“ Ã‰tÃ© 2025. Ces contraintes ont guidÃ© la conception et lâ€™implÃ©mentation du microservice.

## 2.1 Contraintes fonctionnelles

- **Saga orchestrÃ©e synchrone** : La coordination des Ã©tapes dâ€™une commande doit Ãªtre effectuÃ©e de faÃ§on synchrone, selon un enchaÃ®nement bien dÃ©fini (panier â†’ stock â†’ paiement â†’ vente).
- **Rollback partiel obligatoire** : En cas dâ€™Ã©chec Ã  une Ã©tape, le systÃ¨me doit dÃ©clencher des actions de compensation (ex. remboursement, libÃ©ration du stock).
- **Persistance de lâ€™Ã©tat** : Lâ€™Ã©tat courant dâ€™une commande doit Ãªtre sauvegardÃ© dans une base de donnÃ©es, avec un historique des transitions.
- **Consultation de commande** : Il doit Ãªtre possible de consulter lâ€™Ã©tat actuel dâ€™une commande via un endpoint GET.

## 2.2 Contraintes techniques

- **Microservices** : Le service sâ€™intÃ¨gre dans une architecture distribuÃ©e avec des services indÃ©pendants (FastAPI) communiquant via HTTP.
- **Interservices asynchrones (httpx)** : Toutes les communications avec les services distants se font via `httpx.AsyncClient` pour Ã©viter le blocage du serveur.
- **Base de donnÃ©es PostgreSQL** : Le service utilise PostgreSQL pour stocker les commandes (`Commande`) et leur historique (`EtatSaga`).
- **Prometheus pour observabilitÃ©** : Des mÃ©triques par Ã©tat de commande doivent Ãªtre exposÃ©es via `/metrics` (FastAPI Instrumentator).
- **Orchestration centralisÃ©e** : Le service agit comme le coordinateur unique des Ã©tapes, sans attendre dâ€™Ã©vÃ©nement externe (vs chorÃ©graphie).

## 2.3 Contraintes dâ€™infrastructure

- **DÃ©ploiement DockerisÃ©** : Le service doit Ãªtre lancÃ© via `docker-compose` dans un environnement multi-conteneurs.
- **IntÃ©gration Ã  une API Gateway (Krakend)** : Lâ€™orchestrateur doit Ãªtre accessible via la passerelle KrakenD, comme les autres services.
- **Port par dÃ©faut :8000**, rÃ©seau `backend` commun.
- **Monitoring commun avec Prometheus et Grafana**.

## 2.4 Contraintes pÃ©dagogiques

- Le laboratoire vise Ã  simuler une **transaction rÃ©partie robuste** sur plusieurs services, avec journalisation dâ€™Ã©tat et mÃ©canisme de reprise.
- Lâ€™implÃ©mentation de la machine dâ€™Ã©tat est Ã©valuÃ©e, ainsi que sa clartÃ©, sa traÃ§abilitÃ© et sa capacitÃ© Ã  reflÃ©ter les erreurs du systÃ¨me.


# 3. Vue contextuelle

Le `orchestrateur-service` sâ€™inscrit dans une architecture distribuÃ©e orientÃ©e microservices mise en place pour simuler un systÃ¨me de gestion de magasins en ligne. Chaque microservice est responsable dâ€™un domaine mÃ©tier (stock, client, ventes, panier, etc.) et expose ses fonctionnalitÃ©s via des API REST. Lâ€™orchestrateur agit comme **point central de coordination** pour toutes les Ã©tapes critiques dâ€™une commande.

Il ne possÃ¨de pas de logique mÃ©tier propre, mais orchestre celles des autres services selon une sÃ©quence rigide et rÃ©siliente. Il implÃ©mente une saga transactionnelle synchrone qui sâ€™appuie sur des appels HTTP interservices, avec gestion dâ€™erreurs et compensation.

## 3.1 Parties prenantes et environnement

Le systÃ¨me cible deux types dâ€™utilisateurs :
- **Clients** : peuvent crÃ©er un compte, naviguer dans les produits, remplir leur panier et valider une commande.
- **EmployÃ©s** : peuvent gÃ©rer le catalogue de produits, les magasins, les stocks, et consulter les rapports.

Les utilisateurs interagissent avec un **frontend React**, qui communique avec les microservices via une **API Gateway** (Krakend). Tous les services sont dÃ©ployÃ©s dans un environnement DockerisÃ© et communiquent sur le rÃ©seau `backend`.

## 3.2 RÃ´le de lâ€™orchestrateur

Le `orchestrateur-service` ne reÃ§oit des requÃªtes que via un **endpoint unique** (`/checkout`) qui dÃ©clenche la saga. Il interagit ensuite avec les microservices suivants :

- **`panier-service`** : pour lire les produits Ã  commander
- **`stock-service`** : pour rÃ©server ou libÃ©rer le stock
- **`client-service`** : pour effectuer ou rembourser le paiement
- **`ventes-service`** : pour enregistrer la vente

Il maintient en base de donnÃ©es :
- lâ€™Ã©tat courant de la commande (`Commande`)
- lâ€™historique des transitions dâ€™Ã©tat (`EtatSaga`)

Lâ€™API permet aussi de **consulter une commande** pour connaÃ®tre son Ã©tat actuel. Lâ€™orchestrateur expose Ã©galement des **mÃ©triques Prometheus**, utilisÃ©es pour monitorer les transitions dâ€™Ã©tat Ã  des fins dâ€™observabilitÃ©.

## 3.3 Diagramme de contexte

![alt text](image.png)



# 4. Vue logique
![alt text](image-1.png)
Le `orchestrateur-service` est conÃ§u selon une architecture **modulaire et claire**, inspirÃ©e des bonnes pratiques FastAPI. Chaque composant de la logique mÃ©tier est isolÃ© dans son propre dossier : `routers`, `services`, `models`, `schemas`, et `db`.

Lâ€™objectif est de sÃ©parer les responsabilitÃ©s : chaque module a un rÃ´le prÃ©cis dans lâ€™orchestration de la saga.

## 4.1 DÃ©coupage logique

- `routers/commande_orchestration.py` : point dâ€™entrÃ©e HTTP, expose les endpoints `/checkout` et `/commande/{id}`.
- `services/saga_checkout.py` : logique principale dâ€™orchestration, enchaÃ®ne les appels aux services distants via `httpx`.
- `services/etat_commande.py` : fonction `changer_etat_commande` centralisÃ©e pour gÃ©rer les transitions dâ€™Ã©tats, la persistance et les mÃ©triques Prometheus.
- `models/commande.py` : entitÃ© SQLAlchemy persistÃ©e, reprÃ©sente une commande avec son Ã©tat courant.
- `models/etat_saga.py` : entitÃ© SQLAlchemy reprÃ©sentant une transition historique (pour audit).
- `models/etat_commande_enum.py` : Ã©numÃ©ration stricte de tous les Ã©tats possibles dâ€™une commande (de `INITIEE` Ã  `CONFIRMEE` ou `ANNULEE`).
- `schemas.py` : modÃ¨les Pydantic dâ€™entrÃ©e (par exemple `CommandeInput`), utilisÃ©s pour valider les requÃªtes.
- `metrics.py` : compteur Prometheus par Ã©tat de commande (`commande_etat_total`).

## 4.2 Comportement de la saga

La saga suit les Ã©tapes suivantes :
1. **INITIEE** : la commande est crÃ©Ã©e, avec UUID.
2. **RÃ©cupÃ©ration panier** (`panier-service`)
3. **RÃ©servation stock** (`stock-service`)
4. **Paiement** (`client-service`)
5. **Enregistrement vente** (`ventes-service`)
6. **SuccÃ¨s** : `CONFIRMEE`
7. **En cas dâ€™Ã©chec** : rollback stock/paiement + Ã©tat `ANNULEE`

Chaque changement dâ€™Ã©tat est :
- SauvegardÃ© dans `EtatSaga`
- RÃ©percutÃ© dans `Commande.etat_actuel`
- Compteur Prometheus incrÃ©mentÃ©

## 4.3 Avantages de cette structuration

- ModularitÃ© testable et remplaÃ§able
- PossibilitÃ© dâ€™ajouter dâ€™autres Ã©tapes Ã  la saga
- FacilitÃ© dâ€™observabilitÃ© (Prometheus + historique)
- Adaptation Ã  d'autres types dâ€™orchestrations futures (ex : chorÃ©graphiÃ©es)


# 5. Vue de dÃ©veloppement

Le microservice `orchestrateur-service` est structurÃ© selon les bonnes pratiques Python/FastAPI, avec une organisation claire des fichiers, facilitant la maintenabilitÃ©, les tests et lâ€™extension.

## 5.1 Organisation des fichiers

```
orchestrateur-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                      # Point dâ€™entrÃ©e de lâ€™application FastAPI
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ database.py              # Session DB, engine PostgreSQL
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ commande_orchestration.py  # Endpoints: checkout, get commande
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ saga_checkout.py         # Logique principale dâ€™orchestration
â”‚   â”‚   â””â”€â”€ etat_commande.py         # Fonction centrale pour gÃ©rer les transitions
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ commande.py              # ModÃ¨le SQL de la commande
â”‚   â”‚   â”œâ”€â”€ etat_saga.py             # Historique des transitions
â”‚   â”‚   â””â”€â”€ etat_commande_enum.py    # Enum des Ã©tats possibles
â”‚   â”œâ”€â”€ schemas.py                   # ModÃ¨les Pydantic (CommandeInput)
â”‚   â””â”€â”€ metrics.py                   # Exposition des mÃ©triques Prometheus
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ ...

```

## 5.2 Technologies utilisÃ©es

- **FastAPI** : framework web asynchrone
- **SQLAlchemy** : ORM relationnel pour PostgreSQL
- **httpx** : client HTTP asynchrone pour les appels interservices
- **Prometheus FastAPI Instrumentator** : exposition des mÃ©triques
- **Docker** : conteneurisation complÃ¨te
- **PostgreSQL** : persistance locale pour les commandes et transitions

## 5.3 Diagrammes de composants et dâ€™implÃ©mentation

![alt text](image-2.png)

# 6. Vue de dÃ©ploiement

Le `orchestrateur-service` sâ€™inscrit dans une infrastructure Docker multi-conteneurs, interconnectÃ©e Ã  travers un rÃ©seau interne (`backend`) et exposÃ©e via une API Gateway (`krakend-gateway`). Chaque microservice est conteneurisÃ© indÃ©pendamment, facilitant le scaling, le monitoring et la gestion des pannes.

## 6.1 Environnement dâ€™exÃ©cution

- **Conteneur Docker** : Le service tourne dans un container FastAPI (Python 3.11), exposÃ© sur le port `8000`.
- **RÃ©seau Docker `backend`** : utilisÃ© pour la communication avec les autres microservices.
- **PostgreSQL** : chaque microservice possÃ¨de sa propre instance dÃ©diÃ©e de PostgreSQL. Ici, la base contient les tables `commande` et `etat_saga`.
- **Prometheus** : la route `/metrics` permet dâ€™exporter les mÃ©triques du service.
- **Grafana** : connectÃ© Ã  Prometheus pour visualiser lâ€™Ã©tat des commandes.

## 6.2 IntÃ©gration CI/CD (optionnelle)

- Le build se fait via `docker-compose up --build orchestrateur-service`.
- Des tests Postman manuels valident les cas de succÃ¨s/Ã©chec via lâ€™endpoint `/checkout`.

## 6.3 DÃ©pendances rÃ©seau

Le service communique en HTTP asynchrone avec :

- `panier-service` â†’ pour rÃ©cupÃ©rer les produits Ã  commander.
- `stock-service` â†’ pour rÃ©server ou libÃ©rer le stock.
- `client-service` â†’ pour effectuer le paiement ou le remboursement.
- `ventes-service` â†’ pour enregistrer la vente en base.

## 6.4 Diagramme de dÃ©ploiement

![alt text](image-3.png)


## 6.5 Diagramme d'Ã©tats de la saga
![alt text](image-8.png)

# 7. Vue logique

Cette vue dÃ©crit la structure interne du `orchestrateur-service` en termes de **composants logiciels**. Elle prÃ©sente les modules principaux qui composent le service, leurs rÃ´les et leurs interactions.

## 7.1 Composants principaux

Le microservice est structurÃ© selon les conventions FastAPI et respecte une architecture **modulaire inspirÃ©e du 3-tier** :

### `main.py`
Point dâ€™entrÃ©e de lâ€™application. Il instancie lâ€™application FastAPI, inclut les routes et configure les middlewares (dont Prometheus).

### `routers/commande_orchestration.py`
Expose les endpoints HTTP `/api/commande` :
- `POST /checkout` : lance la saga orchestrÃ©e
- `GET /{id}` : rÃ©cupÃ¨re lâ€™Ã©tat courant dâ€™une commande
Il dÃ©lÃ¨gue la logique mÃ©tier au module `services`.

### `services/saga_checkout.py`
Contient la logique de la saga orchestrÃ©e :
- Coordination sÃ©quentielle des appels (panier â†’ stock â†’ client â†’ ventes)
- Gestion des erreurs et compensation
- Logging des transitions dâ€™Ã©tat (`log_etat`)
- IncrÃ©ment des mÃ©triques Prometheus

### `services/etat_commande.py`
Contient la fonction `changer_etat_commande`, qui :
- Met Ã  jour la commande (`etat_actuel`)
- Enregistre la transition dans `EtatSaga`
- IncrÃ©mente la mÃ©trique associÃ©e

### `models/commande.py`, `etat_saga.py`, `etat_commande_enum.py`
DÃ©finissent :
- Le modÃ¨le `Commande` (UUID, client_id, magasin_id, Ã©tat)
- Le modÃ¨le `EtatSaga` (historique de transitions)
- Lâ€™Ã©numÃ©ration `EtatCommande`

### `schemas.py`
Contient le schÃ©ma `CommandeInput` utilisÃ© pour recevoir les requÃªtes utilisateur.

### `metrics.py`
Expose `commande_etat_total` : compteur Prometheus par Ã©tat de commande.

## 7.2 Bases de donnÃ©es

Le service utilise deux tables dans PostgreSQL :
- `commandes` : stocke lâ€™Ã©tat courant
- `etat_saga` : trace lâ€™historique des transitions (avec timestamp)

## 7.3 Diagramme de composants

![alt text](image-4.png)
# 8. Vue comportementale

Cette section illustre le **comportement dynamique** du microservice `orchestrateur-service` Ã  travers des diagrammes de sÃ©quence. Ces diagrammes modÃ©lisent lâ€™enchaÃ®nement des appels lors du traitement dâ€™une commande, ainsi que les alternatives en cas dâ€™Ã©chec.

## 8.1 ScÃ©nario nominal â€“ Traitement complet dâ€™une commande

Ce scÃ©nario illustre le chemin heureux (happy path) dans lequel :
1. Le panier est rÃ©cupÃ©rÃ©,
2. Le stock est rÃ©servÃ©,
3. Le client est dÃ©bitÃ©,
4. La vente est enregistrÃ©e avec succÃ¨s.

Chaque Ã©tape produit une transition de la machine dâ€™Ã©tat (`EtatCommande`) et est journalisÃ©e dans `EtatSaga`.



## 8.2 ScÃ©nario dâ€™Ã©chec â€“ Stock insuffisant

Si le `stock-service` Ã©choue Ã  rÃ©server les produits (ex. stock insuffisant), alors :
- La saga est arrÃªtÃ©e,
- Aucun paiement nâ€™est effectuÃ©,
- Le `EtatCommande` passe Ã  `ECHEC_STOCK`,
- Une entrÃ©e est enregistrÃ©e dans `EtatSaga`.


## 8.3 ScÃ©nario dâ€™Ã©chec â€“ Paiement refusÃ©

Si le paiement Ã©choue :
- Le stock est libÃ©rÃ© (rollback),
- Le client nâ€™est pas facturÃ©,
- Lâ€™Ã©tat passe Ã  `ECHEC_PAIEMENT`.



## 8.4 ScÃ©nario dâ€™Ã©chec â€“ Enregistrement de la vente Ã©chouÃ©

En cas dâ€™Ã©chec lors de lâ€™appel au `ventes-service` :
- Le paiement est remboursÃ©,
- Le stock est libÃ©rÃ©,
- Lâ€™Ã©tat final devient `ECHEC_ENREGISTREMENT`.



---

Chaque scÃ©nario valide le bon fonctionnement de la **compensation transactionnelle**, assurant la cohÃ©rence globale malgrÃ© les pannes partielles.


# 9. Vue de dÃ©veloppement

Cette vue documente la **structure interne** du microservice `orchestrateur-service` en termes de fichiers, modules et dÃ©pendances logicielles. Elle reflÃ¨te lâ€™organisation des dossiers du code, lâ€™architecture logique de FastAPI, et les responsabilitÃ©s attribuÃ©es Ã  chaque couche.

Le projet suit une architecture claire en plusieurs couches :
- `routers/` : DÃ©finition des routes exposÃ©es (ex: `/api/commande`)
- `services/` : Logique mÃ©tier (orchestration, changement dâ€™Ã©tat)
- `models/` : ModÃ¨les SQLAlchemy liÃ©s Ã  la base de donnÃ©es (`Commande`, `EtatSaga`)
- `schemas/` : Objets de validation Pydantic pour les entrÃ©es et sorties API
- `db/` : Configuration de la base de donnÃ©es (session, moteur)
- `metrics.py` : Instrumentation Prometheus
- `main.py` : Point dâ€™entrÃ©e de lâ€™application FastAPI

Cette sÃ©paration respecte les bonnes pratiques SOLID, facilite les tests unitaires, et rend les responsabilitÃ©s explicites.
![alt text](image-5.png)


## 9.2 DÃ©cisions architecturales (ADR)

###  ADR #1 â€“ Adoption dâ€™une orchestration centralisÃ©e pour la gestion des commandes (Saga)

##  Contexte

Dans le cadre du Laboratoire 6 du cours LOG430 (Ã‰tÃ© 2025), nous devions gÃ©rer une transaction rÃ©partie sur plusieurs microservices (panier, stock, client, ventes) de maniÃ¨re fiable, avec rollback en cas dâ€™Ã©chec. Deux approches Ã©taient envisageables : **orchestration centralisÃ©e** ou **chorÃ©graphie distribuÃ©e**.

## DÃ©cision

Nous avons choisi de centraliser la coordination dans un microservice dÃ©diÃ©, appelÃ© `orchestrateur-service`, qui implÃ©mente une **saga orchestrÃ©e synchrone**.

##  ConsÃ©quences

- Un seul point de dÃ©cision contrÃ´le lâ€™ordre et les transitions de la saga.
- Lâ€™**Ã©tat de la commande** est maintenu dans une machine dâ€™Ã©tat persistÃ©e (`etat_actuel` dans `Commande` + historique dans `EtatSaga`).
- Le service `orchestrateur-service` gÃ¨re les appels HTTP interservices, les erreurs, et les actions compensatoires (rollback).
- Chaque Ã©tat est instrumentÃ© avec des mÃ©triques Prometheus (`commande_etat_total`), facilitant le suivi.

## Alternatives envisagÃ©es

- **ChorÃ©graphie distribuÃ©e** : chaque service dÃ©clenche lâ€™Ã©tape suivante via des Ã©vÃ©nements asynchrones.
  - Avantage : dÃ©couplage maximal
  - InconvÃ©nient : complexitÃ© accrue, difficile Ã  suivre et tester
  - NÃ©cessite un bus de messages (RabbitMQ, Kafka)

##  Justification

- Nous voulions maximiser la clartÃ©, la testabilitÃ© et la traÃ§abilitÃ© des transitions.
- Le style synchrone et impÃ©ratif de lâ€™orchestration est mieux adaptÃ© pour un laboratoire avec contraintes de temps, sans infra Kafka.
- Permet dâ€™ajouter facilement des mÃ©triques, des logs, et des tests de bout en bout.

## ğŸ› ï¸ ImplÃ©mentation

- `orchestrateur-service` implÃ©mente la logique complÃ¨te de la saga.
- La progression est journalisÃ©e dans `EtatSaga`, et lâ€™Ã©tat courant dans `Commande`.
- Le tout est exposÃ© via des endpoints REST (`/checkout`, `/commande/{id}`).

---

### ADR #2 â€“ Gestion explicite des Ã©tats de commande via machine dâ€™Ã©tat persistÃ©e

##  Contexte

Dans une saga orchestrÃ©e rÃ©partie sur plusieurs microservices, le suivi de lâ€™Ã©volution de lâ€™Ã©tat de la commande est **critique** pour la cohÃ©rence du systÃ¨me. Il est impÃ©ratif de pouvoir savoir :

- oÃ¹ la commande se trouve dans le processus (stock, paiement, venteâ€¦),
- si un Ã©chec est survenu et Ã  quelle Ã©tape,
- comment effectuer un rollback ou dÃ©clencher une action compensatoire.

##  DÃ©cision

Nous avons mis en place une **machine dâ€™Ã©tat explicite et persistÃ©e**, pilotÃ©e par lâ€™orchestrateur. Deux structures sont utilisÃ©es pour assurer cette gestion :

1. Un **enum** `EtatCommande` dÃ©finit tous les Ã©tats possibles de la commande (initiation, validation, Ã©checs, confirmation).
2. Une **table Commande** contient un champ `etat_actuel` mis Ã  jour Ã  chaque transition.
3. Une **table EtatSaga** conserve lâ€™historique complet des transitions (`saga_id`, `Ã©tat`, `timestamp`).

Chaque transition est aussi **instrumentÃ©e avec Prometheus** pour permettre le monitoring temps rÃ©el de la rÃ©partition des commandes par Ã©tat.

##  ConsÃ©quences

-  **ClartÃ©** : chaque Ã©tat est nommÃ© explicitement et visible via un endpoint `/api/commande/{id}`.
-  **TestabilitÃ©** : permet de valider des cas dâ€™Ã©chec (stock/paiement) en suivant lâ€™Ã©volution des Ã©tats.
-  **ObservabilitÃ©** : les mÃ©triques permettent dâ€™observer le taux dâ€™Ã©checs ou de confirmations dans Grafana.
-  **Persistance** : mÃªme aprÃ¨s redÃ©marrage, lâ€™historique et lâ€™Ã©tat sont disponibles.

##  ImplÃ©mentation

- La fonction `changer_etat_commande(...)` :
  - met Ã  jour la table `Commande` avec lâ€™Ã©tat courant,
  - insÃ¨re un enregistrement dans `EtatSaga`,
  - incrÃ©mente le compteur Prometheus `commande_etat_total{etat="..."}`
- Le service `saga_checkout.py` appelle cette fonction aprÃ¨s chaque Ã©tape rÃ©ussie ou Ã©chouÃ©e.

##  Alternatives considÃ©rÃ©es

-  **Gestion en mÃ©moire** :
  - plus rapide, mais perdue au redÃ©marrage,
  - difficile Ã  tester en environnement distribuÃ©.
-  **Ã‰tat implicite dans les logs ou files dâ€™attente** :
  - difficile Ã  tracer,
  - nÃ©cessite un moteur externe de corrÃ©lation.

##  Justification

- Ce choix rÃ©pond Ã  des objectifs pÃ©dagogiques :
  - mettre en Å“uvre les **principes DDD (Ã©tat explicite, persistÃ©)**,
  - intÃ©grer **lâ€™observabilitÃ©** comme exigence non fonctionnelle (Prometheus, Grafana),
  - permettre une **analyse post-mortem** des commandes Ã©chouÃ©es.

---



# 10. Vue de dÃ©ploiement

Le microservice `orchestrateur-service` est dÃ©ployÃ© dans un environnement **dockerisÃ©**, au sein dâ€™un systÃ¨me distribuÃ© basÃ© sur Docker Compose. Chaque microservice est conteneurisÃ© de maniÃ¨re isolÃ©e, et les communications interservices se font via HTTP sur un rÃ©seau Docker commun (`backend`).

Lâ€™architecture de dÃ©ploiement est composÃ©e des Ã©lÃ©ments suivants :

- **Frontend React** : application web accessible via le navigateur, consommant les API exposÃ©es.
- **KrakenD** (API Gateway) : unique point dâ€™entrÃ©e pour les appels API, rÃ©partissant les requÃªtes vers les services internes.
- **NGINX Load Balancer** : Ã©quilibre la charge entre plusieurs instances de `stock-service`.
- **Microservices FastAPI** :
  - `orchestrateur-service` (coordination de la saga)
  - `panier-service` (panier du client)
  - `stock-service-1` et `stock-service-2` (rÃ©servation/libÃ©ration)
  - `client-service` (paiement/remboursement)
  - `ventes-service` (enregistrement)
  - `produits-service`, `magasin-service`, etc.
- **Bases de donnÃ©es PostgreSQL** : une base par microservice (sauf pour `stock-service` partagÃ© entre instances).
- **Prometheus & Grafana** : outils dâ€™observabilitÃ©, intÃ©grÃ©s dans un rÃ©seau `observability`.

Chaque service expose son port interne (souvent `:8000`) dans son conteneur, mais seul le frontend, la gateway et les outils de monitoring sont exposÃ©s Ã  lâ€™extÃ©rieur.

Cette configuration favorise la rÃ©silience, la montÃ©e en charge et la surveillance prÃ©cise du systÃ¨me.


### 10.3 Capture Grafana â€“ ObservabilitÃ© des Ã©tats de commande

Le tableau de bord Grafana suivant illustre lâ€™activitÃ© de lâ€™orchestrateur dans le temps. Il permet de visualiser :

- Les **points d'entrÃ©e (`handler`) sollicitÃ©s** dans le service (ex. `/api/panier`, `/api/stock/reserver`, `/api/clients`, etc.).
- Le nombre dâ€™appels **par type de requÃªte**, ce qui aide Ã  surveiller la frÃ©quence et la distribution des appels lors des tests ou de la production.
- La **courbe de lâ€™Ã©tat des commandes** (`commande_etat_total`) visible en bas, qui reflÃ¨te :
  - le bon enchaÃ®nement des transitions vers des Ã©tats comme `STOCK_RESERVE`, `PAIEMENT_EFFECTUE`, `CONFIRMEE`,
  - la dÃ©tection des anomalies ou Ã©checs (comme `ECHEC_STOCK`, `ECHEC_PAIEMENT`, etc.).

Cette vue est essentielle pour diagnostiquer les comportements inattendus, vÃ©rifier la robustesse de la saga, dÃ©tecter des goulets dâ€™Ã©tranglement, et confirmer que le systÃ¨me applique correctement les logiques de rollback ou de rÃ©ussite.

![alt text](image-9.png)


# 11. Vue des cas dâ€™utilisation

Le systÃ¨me distribuÃ© multi-magasins vise Ã  couvrir les besoins des **clients** (parcours de commande) et des **employÃ©s** (gestion des produits, magasins, stocks, ventes). Cette section dÃ©crit les principales interactions utilisateur-systÃ¨me en termes de cas d'utilisation, indÃ©pendamment de lâ€™implÃ©mentation technique.

## 11.1 Acteurs

- **Client** : navigue sur lâ€™interface web, consulte les produits, gÃ¨re son panier et effectue des commandes.
- **EmployÃ©** : administre le catalogue, les magasins, les stocks et consulte les rapports de vente.

## 11.2 Cas dâ€™utilisation principaux

### Pour lâ€™employÃ© :
- Ajouter, modifier et consulter les produits
- CrÃ©er et lister les magasins
- Mettre Ã  jour les niveaux de stock
- GÃ©nÃ©rer des rapports de rupture et surstock

### Pour le client :
- CrÃ©er un compte client
- GÃ©rer son panier (ajouter, supprimer des articles)
- Valider sa commande (checkout via saga orchestrÃ©e)

Le processus de **validation de commande (checkout)** est le cas dâ€™utilisation le plus critique : il active la saga distribuÃ©e Ã  travers plusieurs microservices, avec gestion dâ€™Ã©chec et rollback.

---


![alt text](image-6.png)


# 12. Vue logique

La vue logique prÃ©sente les principales entitÃ©s du systÃ¨me et leurs relations dans le domaine de la gestion des commandes orchestrÃ©es. Elle reflÃ¨te les classes et structures utilisÃ©es dans le microservice `orchestrateur-service` ainsi que leurs interactions avec les autres services via appels HTTP.

## 12.1 EntitÃ©s principales

- **Commande** : entitÃ© centrale reprÃ©sentant une commande client. IdentifiÃ©e par un `UUID`, elle contient l'identifiant du client, celui du magasin et son `Ã©tat_actuel`, qui Ã©volue au fil de la saga.
- **EtatCommande (Enum)** : dÃ©finit les Ã©tats possibles dâ€™une commande (INITIÃ‰E, STOCK_RÃ‰SERVÃ‰, PAIEMENT_EFFECTUÃ‰, etc.). UtilisÃ© comme machine dâ€™Ã©tat explicite.
- **EtatSaga** : enregistre lâ€™historique de tous les changements dâ€™Ã©tat dâ€™une commande. Permet la traÃ§abilitÃ© fine du processus.
- **CommandeInput (schema Pydantic)** : objet dâ€™entrÃ©e reÃ§u lors dâ€™une commande (checkout). Contient `client_id` et `magasin_id`.

## 12.2 Relations

- Une **Commande** possÃ¨de un et un seul `etat_actuel`, et peut gÃ©nÃ©rer plusieurs entrÃ©es dans **EtatSaga** au fil de son Ã©volution.
- La structure `EtatCommande` permet de suivre les transitions mÃ©tier et de produire des mÃ©triques par Ã©tat.
- `CommandeInput` est utilisÃ©e uniquement pour encapsuler les donnÃ©es entrantes de l'utilisateur avant orchestration.

## 12.3 Diagramme de classes

Le diagramme ci-dessous illustre les principales entitÃ©s manipulÃ©es dans le service dâ€™orchestration et leur structure interne.

---

![alt text](image-7.png)

# 13. Vue de dÃ©veloppement

La vue de dÃ©veloppement dÃ©crit l'organisation interne du code source du microservice `orchestrateur-service`, en mettant en Ã©vidence les principaux composants (fichiers, modules, packages) et leurs responsabilitÃ©s. Cette vue est essentielle pour comprendre la structure logicielle et les points dâ€™entrÃ©e de lâ€™implÃ©mentation.

## 13.1 Organisation modulaire

Le service suit une architecture modulaire inspirÃ©e de lâ€™architecture hexagonale (ports/adaptateurs) avec les composants suivants :

- **`main.py`** : point dâ€™entrÃ©e de lâ€™application FastAPI. Monte le routeur principal.
- **`routers/commande_orchestration.py`** : contient les endpoints REST exposÃ©s (`/checkout`, `/commande/{id}`).
- **`services/saga_checkout.py`** : implÃ©mente la logique mÃ©tier de la saga orchestrÃ©e : coordination des appels interservices, gestion des Ã©tats, dÃ©clenchement des rollbacks.
- **`services/etat_commande.py`** : gÃ¨re les transitions dâ€™Ã©tat : mise Ã  jour de la base, ajout au journal historique, incrÃ©mentation des mÃ©triques Prometheus.
- **`models/commande.py`** : classe SQLAlchemy reprÃ©sentant la commande (UUID, client, magasin, Ã©tat).
- **`models/etat_saga.py`** : journalisation des transitions sous forme de tuples `(saga_id, Ã©tat, timestamp)`.
- **`models/etat_commande_enum.py`** : Ã©numÃ©ration de tous les Ã©tats possibles (de `INITIÃ‰E` Ã  `ANNULÃ‰E`).
- **`schemas.py`** : modÃ¨les Pydantic (entrÃ©e utilisateur) pour valider les donnÃ©es entrantes.
- **`metrics.py`** : dÃ©finition des compteurs Prometheus (`commande_etat_total`) pour lâ€™observabilitÃ©.
- **`db/database.py`** : gestion de la connexion SQLAlchemy et crÃ©ation de sessions vers PostgreSQL.

## 13.2 Couplage et responsabilitÃ©s

- Le `router` appelle le service `saga_checkout` qui orchestre toute la logique mÃ©tier.
- La persistance est gÃ©rÃ©e dans `etat_commande.py` qui agit comme adaptateur entre logique et stockage.
- Tous les composants mÃ©tiers (modÃ¨les, enums, services) sont dÃ©couplÃ©s et testables individuellement.



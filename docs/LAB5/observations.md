## üîÅ Load Balancing ‚Äì Stock Service

### üéØ Objectif
Mettre en place une r√©partition de charge (`round-robin`) entre deux instances de `stock-service` via un NGINX interm√©diaire, en utilisant KrakenD comme point d'entr√©e unique.

### ‚öôÔ∏è Configuration
- Deux instances Docker (`stock-service-1`, `stock-service-2`)
- Load balancer : `nginx-loadbalancer` (port 8081)
- KrakenD API Gateway (port 8090), route `/api/stock`
- Test de charge : outil [`k6`](https://k6.io)

### üß™ R√©sultat du test de charge
Test effectu√© sur `/api/stock` pendant 1m45 avec mont√©e en charge progressive (1 ‚Üí 100 VUs).
![alt text](image.png)

‚è± Duration : 1m45s
üìà Total requests : 49 736
‚úÖ Succ√®s : 100% (aucune erreur HTTP)
üìâ Latence moyenne: 56.13 ms
üìä Percentiles : 90% sous 131.9 ms / 95% sous 157.3 ms


### üìä Analyse
- ‚úÖ **Stabilit√©** : Aucune requ√™te √©chou√©e m√™me √† 100 VUs ‚Üí bon comportement concurrent.
- ‚ö° **Performance** : Latence raisonnable m√™me √† haut d√©bit (p95 ‚âà 157ms).
- üß† **R√©partition effective** : Le volume trait√© sugg√®re que les deux instances ont bien absorb√© la charge, mais faute de visibilit√© dans Prometheus/Grafana (bug DNS), la r√©partition exacte n‚Äôa pas pu √™tre observ√©e visuellement.

### ‚ö†Ô∏è Limites rencontr√©es
- ‚ùå **Prometheus/Grafana** : √âchec de la configuration des targets `stock-service-1` et `stock-service-2` (erreur DNS), donc m√©triques non disponibles pour visualisation directe.
- üõ† **Suggestion** : Ajouter les deux instances dans le `docker-compose` avec le m√™me nom de `job` mais IP diff√©rente ou corriger la configuration DNS du r√©seau `log430-network`.
![alt text](image-1.png)
### ‚úÖ Conclusion
Le test de charge d√©montre que le **load balancing fonctionne correctement en pratique**. M√™me sans visualisation Prometheus, la **r√©ponse rapide, stable et continue** confirme que les deux instances √©taient actives et sollicit√©es sous charge.

# üî¨ Comparaison des Architectures ‚Äì Avant vs Apr√®s (Lab 5 ‚Äì LOG430)

Dans cette section, nous comparons les r√©sultats **avant** et **apr√®s** la migration vers une architecture microservices avec **API Gateway (KrakenD)** et **Load Balancer (NGINX)**.

---

## ‚öñÔ∏è Comparaison Architecture Monolithique vs Microservices

### üîÅ Sc√©narios Test√©s

| Sc√©nario | D√©tail |
|---------|--------|
| üß± **Monolithique** | Appels directs √† FastAPI (`http://localhost:8000`) |
| üß© **Microservices + Gateway** | Appels via KrakenD Gateway (`http://localhost:8090`) et NGINX Load Balancer (`http://localhost:8081`) |

---

## üìà R√©sultats ‚Äì Tests de Charge (via K6)

### üß™ Test 1 ‚Äì Appel Direct (ancienne architecture)

```bash
k6 run test.js # vers http://localhost:8000/api/magasins/1/stock
```

- ‚úÖ **Requ√™tes r√©ussies** : 49‚ÄØ736
- ‚è±Ô∏è **Latence moyenne** : ~54 ms
- üìâ **Max latence (p95)** : ~152 ms
- ‚ùå **Taux d‚Äôerreurs** : 0 %

### üß™ Test 2 ‚Äì Appel via Gateway (nouvelle architecture)

```bash
k6 run test.js # vers http://localhost:8090/api/magasins/1/stock
```

- ‚úÖ **Requ√™tes r√©ussies** : 49‚ÄØ736
- ‚è±Ô∏è **Latence moyenne** : ~56 ms
- üìâ **Max latence (p95)** : ~157 ms
- ‚ùå **Taux d‚Äôerreurs** : 0 %

---

## üßæ Tableau Comparatif

| Crit√®re                   | Ancienne Architecture       | Nouvelle Architecture (Gateway + Load Balancer) |
|---------------------------|-----------------------------|--------------------------------------------------|
| üîÅ **Latence (moyenne)**    | ~54 ms                      | ~56 ms                                           |
| ‚úÖ **Disponibilit√©**        | Haute                       | Haute (tol√©rance aux pannes am√©lior√©e)           |
| üìä **Tra√ßabilit√©**          | Limit√©e                     | Am√©lior√©e via Gateway (logging, quotas)          |
| üß± **Simplicit√©**           | ‚úÖ Simple                    | ‚ùå Plus complexe (d√©marrage + config)             |
| üîç **Visibilit√©**           | Faible (1 seul /metrics)    | Grande visibilit√© par service (`/metrics`)       |
| üîÅ **Scalabilit√© horizontale** | ‚ùå Non support√©e             | ‚úÖ Load balancer actif sur 2 instances `stock-service` |
| üîê **S√©curit√©**             | Basique                     | CORS, tokens, throttling activ√©s dans la Gateway |
| üîß **Test de panne**        | ‚ùå Pas tol√©rant              | ‚úÖ R√©silient (r√©partition automatique du trafic)  |

---

## üß† Analyse

- üìâ **Latence l√©g√®rement sup√©rieure** en microservices (normal √† cause des hops r√©seau + Gateway)
- ‚úÖ **Stabilit√© accrue** : pas de 5xx, r√©silience confirm√©e m√™me en surcharge
- üîç **Visibilit√© fine** par service gr√¢ce √† Prometheus + Grafana
- üîß **Possibilit√©s avanc√©es** : quotas, cache, routing dynamique via Gateway
- ‚ùóÔ∏è **Complexit√© accrue** : plus de fichiers √† maintenir (`docker-compose`, `krakend.json`, `nginx.conf`)

---

## üìä Dashboard Grafana

Les dashboards utilis√©s contiennent :

- Latence (avg / p95)
- Requ√™tes par seconde (RPS)
- Erreurs (4xx / 5xx)
- Saturation CPU
- Distribution de charge (stock-service-1 / stock-service-2)
- Impact du cache

> üì∏ Captures int√©gr√©es dans `docs/LAB5/images` :
- `latence_compare.png`
- `charge_balancer.png`
- `pannes_failover.png`
- `strategie_routing.png`

---

## ‚úÖ Conclusion

La nouvelle architecture offre une **visibilit√© accrue**, une **meilleure tol√©rance aux pannes**, et une **scalabilit√© horizontale**. Malgr√© une l√©g√®re hausse de la latence, les b√©n√©fices en termes de **r√©silience, observabilit√© et flexibilit√©** justifient la complexit√© ajout√©e.